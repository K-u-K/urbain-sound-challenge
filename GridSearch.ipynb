{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GridSearch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUojL_DcLpOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98aEackSM3a5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Flatten, Conv1D, LeakyReLU, BatchNormalization\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCbJYYK8Ndga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# paths\n",
        "directory = \"drive/My Drive/UE_Proell\"\n",
        "preprocessed = f\"{directory}/preprocessed\"\n",
        "gridsearch = f\"{directory}/gridsearch\"\n",
        "\n",
        "# hyper parameter\n",
        "loss = 'categorical_crossentropy'\n",
        "leaky_alpha = 0.01\n",
        "epochs = 150\n",
        "\n",
        "learning_rates = [0.01, 0.001]\n",
        "dropout_rates  = [0.2, 0.5]\n",
        "batch_sizes    = [10, 20, 40]\n",
        "\n",
        "neurons    = [128, 256, 512]\n",
        "kernels    = [3, 5, 7]\n",
        "lstm_units = [16, 32, 48]\n",
        "\n",
        "# configuration parameter\n",
        "validation_split = 0.2\n",
        "patience = 5\n",
        "verbose = 0\n",
        "seed = 42\n",
        "\n",
        "# labels\n",
        "labels = [\n",
        "    \"air conditioner\",\n",
        "    \"car horn\",\n",
        "    \"children playing\",\n",
        "    \"dog bark\",\n",
        "    \"drilling\",\n",
        "    \"engine idling\",\n",
        "    \"gun shot\",\n",
        "    \"jackhammer\",\n",
        "    \"siren\",\n",
        "    \"street music\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaWIPFmrO5H4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.makedirs(gridsearch, exist_ok=True)\n",
        "\n",
        "X_train, y_train = np.load(f\"{preprocessed}/X_train.npy\"), np.load(f\"{preprocessed}/y_train.npy\")\n",
        "X_test, y_test = np.load(f\"{preprocessed}/X_test.npy\"), np.load(f\"{preprocessed}/y_test.npy\")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=validation_split, random_state=seed, stratify=y_train)\n",
        "num_of_labels = y_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsLyMjxwOVvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDNNs(input_shape, dropout):\n",
        "  dnns = []\n",
        "  for neuron in neurons:\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neuron, input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(num_of_labels, activation='softmax'))\n",
        "    dnns.append({\n",
        "        \"name\": f\"l1_{neuron}_relu\", \"model\": model\n",
        "    })\n",
        "  for n1, n2 in itertools.product(neurons, neurons):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(n1, input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(n2, activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(num_of_labels, activation='softmax'))\n",
        "    dnns.append({\n",
        "        \"name\": f\"l2_{n1}_{n2}_relu\", \"model\": model\n",
        "    })\n",
        "  return dnns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjueF_oAhRsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getCNNs(input_shape, dropout):\n",
        "  cnns = []\n",
        "  for neuron in neurons:\n",
        "    for kernel in kernels:\n",
        "      model = Sequential()\n",
        "      model.add(Conv1D(filters = 1, kernel_size = kernel, input_shape=input_shape))\n",
        "      model.add(LeakyReLU(alpha = leaky_alpha))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Flatten())\n",
        "      model.add(Dropout(dropout))\n",
        "      model.add(Dense(neuron, activation='relu'))\n",
        "      model.add(Dense(num_of_labels, activation='softmax'))\n",
        "      cnns.append({\n",
        "          \"name\": f\"l1_k{kernel}_d{neuron}_relu\", \"model\": model\n",
        "      })\n",
        "    for k1, k2 in itertools.product(kernels, kernels):\n",
        "      model = Sequential()\n",
        "      model.add(Conv1D(filters = 1, kernel_size = k1, input_shape=input_shape))\n",
        "      model.add(LeakyReLU(alpha = leaky_alpha))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Conv1D(filters = 1, kernel_size = k2))\n",
        "      model.add(LeakyReLU(alpha = leaky_alpha))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Flatten())\n",
        "      model.add(Dropout(dropout))\n",
        "      model.add(Dense(neuron, activation='relu'))\n",
        "      model.add(Dense(num_of_labels, activation='softmax'))\n",
        "      cnns.append({\n",
        "          \"name\": f\"l2_k{k1}_k{k2}_d{neuron}_relu\", \"model\": model\n",
        "      })\n",
        "  return cnns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRYS4XFEifPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getLSTMs(input_shape, dropout):\n",
        "  lstms = []\n",
        "  for lstm in lstm_units:\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(lstm, input_shape=input_shape))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(num_of_labels, activation='softmax'))\n",
        "    lstms.append({\n",
        "        \"name\": f\"l1_lstm{lstm}_relu\", \"model\": model\n",
        "    })\n",
        "  return lstms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MDfL-nQuvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_methods = {\n",
        "    \"dnn\": {\n",
        "        \"method\": lambda input_shape: getDNNs((40, ), dropout),\n",
        "        \"tests\": (len(neurons) + len(neurons) * len(neurons))\n",
        "    },\n",
        "    \"cnn\": {\n",
        "         \"method\": lambda input_shape: getCNNs((40, 1), dropout),\n",
        "         \"tests\":  (len(neurons) * len(kernels) + len(neurons) * len(kernels) * len(kernels))\n",
        "    }\n",
        "    \"lstm\": {\n",
        "       \"method\": lambda input_shape: getLSTMs((1, 40), dropout),\n",
        "       \"tests\":  (len(lstm_units))\n",
        "    }\n",
        "}\n",
        "\n",
        "for network, n_dict in nn_methods.items():\n",
        "  results = []\n",
        "\n",
        "  counter = 1\n",
        "  tests   = len(learning_rates) * len(dropout_rates) * len(batch_sizes) * n_dict[\"tests\"]\n",
        "\n",
        "  for learning_rate, dropout, batch_size in itertools.product(learning_rates, dropout_rates, batch_sizes):\n",
        "    nns = n_dict[\"method\"](dropout)\n",
        "\n",
        "    for nn in nns:\n",
        "      architecture = nn[\"name\"]\n",
        "\n",
        "      print(f\"[{network}][{counter}/{tests}] Train network {network} -- {architecture} -- lr {learning_rate} -- d {dropout} -- bz {batch_size}\")\n",
        "      counter = counter + 1\n",
        "\n",
        "      model = nn[\"model\"]\n",
        "      model.compile(Adam(lr=learning_rate), loss=loss, metrics=['accuracy'])\n",
        "\n",
        "      callbacks = [\n",
        "            EarlyStopping(monitor=\"loss\", patience=patience, verbose=verbose, mode=\"auto\"),\n",
        "      ]\n",
        "      \n",
        "      X = {\n",
        "          \"train\": X_train.copy(),\n",
        "          \"test\": X_test.copy(),\n",
        "          \"valid\": X_valid.copy()\n",
        "      }\n",
        "\n",
        "      if network == \"cnn\":\n",
        "        X[\"train\"] = np.expand_dims(X[\"train\"], axis=2)\n",
        "        X[\"test\"] = np.expand_dims(X[\"test\"], axis=2)\n",
        "        X[\"valid\"] = np.expand_dims(X[\"valid\"], axis=2)\n",
        "      if network == \"lstm\":\n",
        "        X[\"train\"] = np.expand_dims(X[\"train\"], axis=1)\n",
        "        X[\"test\"] = np.expand_dims(X[\"test\"], axis=1)\n",
        "        X[\"valid\"] = np.expand_dims(X[\"valid\"], axis=1)\n",
        "\n",
        "      hist = model.fit(X[\"train\"], y_train, \n",
        "                      validation_data=(X[\"valid\"], y_valid), \n",
        "                      batch_size=batch_size, \n",
        "                      epochs=epochs, \n",
        "                      shuffle=True, \n",
        "                      verbose=verbose, \n",
        "                      callbacks=callbacks)\n",
        "      \n",
        "      scores   = model.evaluate(X[\"test\"], y_test, verbose=1)\n",
        "      accuracy = scores[1] * 100\n",
        "      print(f\"\\ttestaccuracy {accuracy}\")\n",
        "\n",
        "      y_pred_indices = np.argmax(model.predict(X[\"test\"]), axis=1)\n",
        "      y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "      report = metrics.classification_report(y_test_indices, y_pred_indices, labels=np.unique(y_test_indices), digits=3, output_dict=True)\n",
        "      results.append({\n",
        "          \"network\":       network,\n",
        "          \"architecture\":  architecture,\n",
        "          \"learning_rate\": learning_rate,\n",
        "          \"dropout\":       dropout,\n",
        "          \"batch_size\":    batch_size,\n",
        "          \"accuracy\":      accuracy,\n",
        "          \"precision\":     report[\"weighted avg\"][\"precision\"],\n",
        "          \"recall\":        report[\"weighted avg\"][\"recall\"],\n",
        "          \"f1-score\":      report[\"weighted avg\"][\"f1-score\"]\n",
        "      })\n",
        "\n",
        "  with open(f\"{gridsearch}/gs_{network}.json\", \"w\") as fh:\n",
        "    json.dump(results, fh, indent=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}